{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "path = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('{}file-name_query03-05.parquet'.format(path))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = pd.read_parquet('{}topics03-05.parquet'.format(path))\n",
    "texts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def create_phrases_from_sentences(text):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    phrases = []\n",
    "    current_phrase = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # If single sentence exceeds 512, keep it as a whole phrase\n",
    "        if sentence_length > 512:\n",
    "            if current_phrase:  # Store any accumulated phrases first\n",
    "                phrases.append(' '.join(current_phrase))\n",
    "                current_phrase = []\n",
    "                current_length = 0\n",
    "            phrases.append(sentence)\n",
    "            continue\n",
    "            \n",
    "        # Check if adding this sentence would exceed 512 characters\n",
    "        if current_length + sentence_length + 1 <= 512:  # +1 for space\n",
    "            current_phrase.append(sentence)\n",
    "            current_length += sentence_length + 1\n",
    "        else:\n",
    "            # Store current phrase and start new one\n",
    "            if current_phrase:\n",
    "                phrases.append(' '.join(current_phrase))\n",
    "            current_phrase = [sentence]\n",
    "            current_length = sentence_length\n",
    "    \n",
    "    # Add any remaining phrases\n",
    "    if current_phrase:\n",
    "        phrases.append(' '.join(current_phrase))\n",
    "    \n",
    "    return phrases\n",
    "\n",
    "# Apply the function to create phrases_2 column\n",
    "texts_df['phrases_2'] = texts_df['text'].apply(create_phrases_from_sentences)\n",
    "texts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "# Load Sentence model (based on BERT) from URL\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "def model_encode(model):\n",
    "    embeddings = []\n",
    "    for _, phrases in tqdm(texts_df['phrases'].items()):\n",
    "        embedding = np.zeros(512)\n",
    "        for phrase in phrases:\n",
    "            embedding += model.encode(phrase, show_progress_bar=False)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "texts_df['embedding'] = model_encode(model)\n",
    "texts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.to_parquet('{}topics_embeddings.parquet'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_element_positions(phrase, element):\n",
    "    \"\"\"Find character and word positions of an element in a phrase.\"\"\"\n",
    "    if not isinstance(element, str) or pd.isna(element) or not element.strip():\n",
    "        return None\n",
    "    \n",
    "    # Convert both to lowercase for case-insensitive matching\n",
    "    phrase_lower = phrase.lower()\n",
    "    element_lower = element.lower()\n",
    "    \n",
    "    # Find character position\n",
    "    char_pos = phrase_lower.find(element_lower)\n",
    "    if char_pos == -1:\n",
    "        return None\n",
    "        \n",
    "    # Find word position\n",
    "    words = phrase[:char_pos].split()\n",
    "    start_word_pos = len(words)\n",
    "    end_word_pos = start_word_pos + len(element_lower.split())\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'surface_char_pos': np.array([char_pos, char_pos + len(element_lower)]),\n",
    "        'surface_form': element.lower(),\n",
    "        'surface_word_pos': np.array([start_word_pos, end_word_pos]),\n",
    "        'types': np.array([], dtype=object),\n",
    "        'uri': element.lower()\n",
    "    }\n",
    "\n",
    "def process_entities(df, texts_df):\n",
    "    # Merge the dataframes on doi\n",
    "    merged_df = pd.merge(texts_df, df, on='doi', how='left')\n",
    "    \n",
    "    # Columns to check for entities\n",
    "    entity_columns = ['bioActivity', 'collectionSpecie', 'collectionSite', \n",
    "                     'collectionType', 'name']\n",
    "    \n",
    "    data = []\n",
    "    data_2 = []\n",
    "    \n",
    "    # Global counter for unique phrase IDs\n",
    "    global_phrase_idx = 0\n",
    "    \n",
    "    # Iterate through each row\n",
    "    for idx, row in merged_df.iterrows():\n",
    "        if not isinstance(row['phrases_2'], list):\n",
    "            continue\n",
    "            \n",
    "        # Process each phrase\n",
    "        for phrase in row['phrases_2']:\n",
    "            if not isinstance(phrase, str):\n",
    "                continue\n",
    "                \n",
    "            phrase = re.sub(\"(\\\\d|\\\\W)+\",\" \",phrase)\n",
    "            phrase = phrase.strip()\n",
    "\n",
    "            data_2.append({\n",
    "                'label': global_phrase_idx,\n",
    "                'text_a': phrase.lower()\n",
    "            })\n",
    "                \n",
    "            # Check each entity column\n",
    "            for col in entity_columns:\n",
    "                element = row[col]\n",
    "                element = re.sub(\"(\\\\d|\\\\W)+\",\" \",element)\n",
    "                element = element.strip()\n",
    "                positions = find_element_positions(phrase, element)\n",
    "                \n",
    "                if positions:\n",
    "                    data.append({\n",
    "                        'source_id': global_phrase_idx,\n",
    "                        'entity_data': positions,\n",
    "                        'uri': element.lower()\n",
    "                    })\n",
    "            \n",
    "            # Increment the global counter after processing each phrase\n",
    "            global_phrase_idx += 1\n",
    "    \n",
    "    return data, data_2\n",
    "\n",
    "# Example usage:\n",
    "data, data_2 = process_entities(df, texts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.DataFrame(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.to_parquet('../kims-bert/CODE/K-BERT/datasets/scholarly_dataset/scholarly__3000__train__entity_mapping_by_sentence.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.to_csv(\"../kims-bert/CODE/K-BERT/datasets/scholarly_dataset/scholarly__3000__train.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "# Create lists to store the SPO triples\n",
    "spo_triples = []\n",
    "\n",
    "# Function to add triple if value exists\n",
    "def add_triple_if_exists(subject, predicate, object_value):\n",
    "    if pd.notna(object_value) and object_value != \"\":\n",
    "        spo_triples.append(f\"{subject}\\t{predicate}\\t{object_value}\")\n",
    "\n",
    "# Process each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    doi = row['doi']\n",
    "\n",
    "\n",
    "    row['bioActivity'] = re.sub(\"(\\\\d|\\\\W)+\",\" \",row['bioActivity'])\n",
    "    row['collectionSpecie'] = re.sub(\"(\\\\d|\\\\W)+\",\" \",row['collectionSpecie'])\n",
    "    row['collectionSite'] = re.sub(\"(\\\\d|\\\\W)+\",\" \",row['collectionSite'])\n",
    "    row['collectionType'] = re.sub(\"(\\\\d|\\\\W)+\",\" \",row['collectionType'])\n",
    "    row['name'] = re.sub(\"(\\\\d|\\\\W)+\",\" \",row['name'])\n",
    "\n",
    "    # Clean and trim each field\n",
    "    row['bioActivity'] = row['bioActivity'].strip()\n",
    "    row['collectionSpecie'] = row['collectionSpecie'].strip()\n",
    "    row['collectionSite'] = row['collectionSite'].strip()\n",
    "    row['collectionType'] = row['collectionType'].strip()\n",
    "    row['name'] = row['name'].strip()\n",
    "\n",
    "    \n",
    "    # Add triples for each specified relationship\n",
    "    add_triple_if_exists(row['bioActivity'].lower(), \"bioActivity\", doi)\n",
    "    add_triple_if_exists(row['collectionSpecie'].lower(), \"collectionSpecie\", doi)\n",
    "    add_triple_if_exists(row['collectionSite'].lower(), \"collectionSite\", doi)\n",
    "    add_triple_if_exists(row['collectionType'].lower(), \"collectionType\", doi)\n",
    "    add_triple_if_exists(row['name'].lower(), \"name\", doi)\n",
    "\n",
    "# Write to .spo file\n",
    "with open('../kims-bert/CODE/K-BERT/brain/kgs/KG_3cols.spo', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"sub\\tpred\\tobj\\n\")  # Header\n",
    "    f.write('\\n'.join(spo_triples))\n",
    "\n",
    "print(\"Knowledge graph has been created in knowledge_graph.spo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb93546e6995fdb0878df83f1978a6e198dddb57421150bbcbdd2209f86aef4c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
